import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from tqdm import tqdm

WAYBACK_API = "http://web.archive.org/cdx/search/cdx"

# Regex de LinkFinder para encontrar URLs con parámetros
URL_PARAM_REGEX = r"[\"']((?:https?:)?\/\/[^\"']+\?[^\s\"'>]+)[\"']"

def get_wayback_urls(domain):
    print(f"[*] Buscando en Wayback Machine: {domain}")
    try:
        response = requests.get(WAYBACK_API, params={
            "url": domain + "/*",
            "output": "json",
            "fl": "original",
            "collapse": "urlkey"
        }, timeout=10)
        response.raise_for_status()  # Lanza una excepción para códigos de estado no 2xx
        if response.status_code == 200:
            urls = list(set([entry[0] for entry in response.json()[1:]]))
            return urls
    except requests.exceptions.RequestException as e:
        # Capturamos todos los errores relacionados con solicitudes HTTP
        print(f"[!] Error en Wayback Machine para la URL {domain}: {e}")
    except Exception as e:
        # Capturamos cualquier otro error inesperado
        print(f"[!] Error desconocido en Wayback Machine para la URL {domain}: {e}")
    return []  # Retorna una lista vacía si hubo un error

def crawl_site(url):
    print(f"[*] Crawling en sitio: {url}")
    visited = set()
    urls = set()
    queue = [url]

    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    for _ in tqdm(range(50), desc="Crawling"):
        if not queue:
            break
        current = queue.pop(0)
        if current in visited:
            continue
        visited.add(current)
        try:
            res = requests.get(current, headers=headers, timeout=5)
            res.raise_for_status()  # Lanza una excepción si la respuesta no es 2xx
            base = res.url
            soup = BeautifulSoup(res.text, "html.parser")
            for tag in soup.find_all(["a", "script", "link", "iframe"]):
                attr = tag.get("href") or tag.get("src")
                if attr:
                    full_url = urljoin(base, attr)
                    if urlparse(full_url).netloc == urlparse(url).netloc:
                        if full_url not in visited:
                            queue.append(full_url)
                            urls.add(full_url)
            # Extraer también con regex desde el HTML/JS
            found = re.findall(URL_PARAM_REGEX, res.text)
            for f in found:
                if "?" in f:
                    full = f if f.startswith("http") else urljoin(base, f)
                    urls.add(full)
        except requests.exceptions.RequestException as e:
            print(f"[!] Error de red al acceder a {current}: {e}")
        except Exception as e:
            print(f"[!] Error al procesar {current}: {e}")
            continue  # Continua con la siguiente URL si ocurre cualquier otro error

    return list(urls)

def is_same_domain(base_url, test_url):
    base_netloc = urlparse(base_url).netloc
    test_netloc = urlparse(test_url).netloc
    return test_netloc.endswith(base_netloc)

def extract_params(target_url):
    all_urls = set()

    # Agregar desde Wayback
    wayback_urls = get_wayback_urls(target_url)
    if wayback_urls:
        all_urls.update(wayback_urls)
    else:
        print(f"[!] No se pudieron obtener URLs de Wayback para {target_url}")

    # Agregar desde crawling
    crawled_urls = crawl_site(target_url)
    if crawled_urls:
        all_urls.update(crawled_urls)
    else:
        print(f"[!] No se pudieron obtener URLs al hacer crawling en {target_url}")

    print(f"\n[+] URLs totales encontradas: {len(all_urls)}")

    # Filtrar URLs con parámetros válidas dentro del mismo dominio/subdominio
    urls_with_params = [
        u for u in all_urls 
        if "?" in u and "=" in u and is_same_domain(target_url, u)
    ]

    # Deduplicar por base + nombre de parámetro
    unique_param_urls = []
    seen_param_signatures = set()
    for url in urls_with_params:
        parsed = urlparse(url)
        base = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        query = parsed.query.split('&')
        for param in query:
            key = param.split("=")[0]
            signature = f"{base}?{key}="
            if signature not in seen_param_signatures:
                seen_param_signatures.add(signature)
                # reconstruimos la URL con solo ese parámetro
                clean_url = f"{base}?{param}"
                unique_param_urls.append(clean_url)

    print(f"[+] URLs con parámetros únicos: {len(unique_param_urls)}")

    for u in unique_param_urls:
        print(f"[{CYAN}+{RESET}] Param URL found: {u}")

    print(f"[+] Se encontraron {len(unique_param_urls)} URLs con parámetros únicas.\n")
    return unique_param_urls


# Colores para uso local
CYAN = "\033[96m"
RESET = "\033[0m"
